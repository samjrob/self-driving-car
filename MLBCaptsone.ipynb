{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1BGHkqQs2w0z",
        "XbcPrjFV3biW",
        "-15y4jtR3KUX",
        "wF5wi89hTb39"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Self-Driving Car\n",
        "This Jupiter notebook details my approach to developing a program for a self-driving car. Using dashcam footage the program is able to determine how fast the car is moving in m/s and identify street signs on the sides of the road."
      ],
      "metadata": {
        "id": "zQbC52VCF9lW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbMPcKmwpPIw"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounts my google drive for access to important files for training\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgKM3c5-W2qH",
        "outputId": "fe915058-37ff-4808-f68a-cbdca4d7fa78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Speed Determination\n",
        "To begin I will create a model that takes two images and determines the velocity of the car using the change between them. This implementation utilizes cv2 and optical flow to create an input image that shows the color-coded apparent movement of objects between the two frames. These images are then paired with a label that is the average velocity of the two frames used for the optical flow. The paired images and labels are then used to train a convolutional neural network. This trained network is able to take in optical flows created from two frames and estimate the velocity of the car.\n",
        "\n",
        "Data From: https://github.com/commaai/speedchallenge"
      ],
      "metadata": {
        "id": "1BGHkqQs2w0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign files to variable names\n",
        "speed_training_data = '/content/train.mp4'\n",
        "speed_labels = '/content/train.txt'\n",
        "speed_testing_data = '/content/test.mp4'\n",
        "final_vid = '/content/final_input.mp4'"
      ],
      "metadata": {
        "id": "rv3W3fozpfng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "epochs = 5\n",
        "batch_size = 10\n",
        "WIDTH  = 128\n",
        "HEIGHT = 96"
      ],
      "metadata": {
        "id": "X3OycWmMOpXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the difference between two frames and assign the average velocity between these two\n",
        "# frames as the label velocity for the given difference. Takes in the video file and velocity\n",
        "# labels for each frame as parameters\n",
        "def labeled_frame(speed_training_data, speed_labels):\n",
        "  v_tot = 0\n",
        "  cap = cv2.VideoCapture(speed_training_data)\n",
        "  f = open(speed_labels, \"r\")\n",
        "  frame_change = []\n",
        "  label = []\n",
        "  while True:\n",
        "    ret, frame1 = cap.read() # Get the first frame using cv2\n",
        "    ret, frame2 = cap.read() # Get the second frame\n",
        "    if frame1 is None: # Exit loop once out of frames\n",
        "      break\n",
        "\n",
        "    # Retrieve the velocity labels for each frame and average them\n",
        "    for i in range(2):\n",
        "      v_tot += float(f.readline())\n",
        "    ave_v = v_tot / 2 # Average the two velocity labels for the frames\n",
        "    label.append(ave_v)\n",
        "    v_tot = 0\n",
        "\n",
        "    color_coded = frames_to_optical_flow(frame1, frame2)\n",
        "\n",
        "    frame_change.append(color_coded)\n",
        "\n",
        "  # Close the cv2 capture and file\n",
        "  f.close()\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  return (frame_change, label)\n"
      ],
      "metadata": {
        "id": "Pgijh27Frfx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frames_to_optical_flow(frame1, frame2):\n",
        "  # Utilize cv2's optical flow, which will the change between frames and give a vector\n",
        "  # value, which then can be converted to colors using HSV\n",
        "  gray1 = cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY) # Grayscale the frames for optical flow\n",
        "  gray2 = cv2.cvtColor(frame2,cv2.COLOR_BGR2GRAY)\n",
        "  optical_flow = cv2.calcOpticalFlowFarneback(gray1,gray2, None, 0.5, 1, 10, 3, 5, 1.1, 0) # optical flow parametes set according to cv2 documentation\n",
        "\n",
        "  # Utilizing HSV we can color code the direction and magnitude of the change. H as the angle will determine the color, S will always be maxed\n",
        "  # since we want to see the colors, and V will represent the magnitude, brighter color correlates to a greater change\n",
        "  hsv = np.zeros_like(frame1) # Create empty array with the same chape as the frame\n",
        "  hsv[...,1] = 255 # Set the s column to 100%\n",
        "  magnitude, angle = cv2.cartToPolar(optical_flow[...,0], optical_flow[...,1], angleInDegrees=True) # cartToPolar takes x and y coordinate and\n",
        "                                                                                                        # returns them as polar coordinates\n",
        "  hsv[...,0] = angle / 2 # Angle correlates to which color. Divide by 2 so value does not exceed 255\n",
        "  hsv[...,2] = cv2.normalize(magnitude,None, 0, 255, cv2.NORM_MINMAX) # Normalize magnitude to values between 0 and 255 for proper mapping to RGB\n",
        "\n",
        "  # Need to convert the output to RGB for color coding\n",
        "  color_coded = cv2.cvtColor(hsv,cv2.COLOR_HSV2BGR) # Color code\n",
        "\n",
        "  # Shrink output image for NN according to documentation\n",
        "  color_coded = cv2.resize(color_coded, (WIDTH, HEIGHT), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "  return color_coded"
      ],
      "metadata": {
        "id": "Of2jD0qN2Ddj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_speed_model():\n",
        "  input_shape = (HEIGHT, WIDTH, 3)\n",
        "\n",
        "  # Network structure motivated by NVIDIA model designed for self-driving cars, adapted for smaller scale\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape)) # normalize inputs\n",
        "\n",
        "  # Small Conv2D CNN layer to pick up on basic features\n",
        "  model.add(tf.keras.layers.Conv2D(24, (5, 5), activation='elu', strides=(2, 2)))\n",
        "  model.add(tf.keras.layers.Conv2D(36, (5, 5), activation='elu', strides=(2, 2)))\n",
        "  model.add(tf.keras.layers.Conv2D(48, (5, 5), activation='elu', strides=(2, 2)))\n",
        "\n",
        "  # Dropout layer to avoid local minima\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  # Larger Conv2D CNN layer with smaller stride to pick up on more advanced features\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu', strides=(1, 1)))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu', strides=(1, 1)))\n",
        "\n",
        "  # Flatten for fully connected NN\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  # Fully connected NN, to last layer with one output\n",
        "  model.add(tf.keras.layers.Dense(100, activation='elu'))\n",
        "  model.add(tf.keras.layers.Dense(50, activation='elu'))\n",
        "  model.add(tf.keras.layers.Dense(10, activation='elu'))\n",
        "  model.add(tf.keras.layers.Dense(1, name='output'))\n",
        "\n",
        "  # Compile the model with ADAM\n",
        "  model.compile(optimizer='adam', loss=\"mse\")\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "4-EubvYf3uLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve a list of optical flow frames and their associated speed labels\n",
        "images, speed_label = labeled_frame(speed_training_data, speed_labels)"
      ],
      "metadata": {
        "id": "vj8gtQwfkIQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the frames into train and test sets, with 80% of the data for training, then create the model\n",
        "x_train, x_test, y_train, y_test = train_test_split(np.array(images), np.array(speed_label), test_size=0.2, shuffle=True)\n",
        "speed_model = create_speed_model()"
      ],
      "metadata": {
        "id": "h5jA-Z4pNBmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the training data, save metrics to a varible called history\n",
        "history = speed_model.fit(x = x_train, y = y_train, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "id": "T6LvbkLC5NIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to the SpeedModel folder of my google drive for future access\n",
        "speed_model.save('/content/gdrive/MyDrive/SpeedModel/speed_model.keras')"
      ],
      "metadata": {
        "id": "1UuolViyTfOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate and graph the MSE of the speed training model\n",
        "speed_model.evaluate(x = x_test, y = y_test, batch_size=batch_size)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('mse loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LTaDAR0Y09m7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying Street Signs\n",
        "\n",
        "Next, I will be training the YOLOv3 object detection model on a custom dataset of German street signs. The model will take in any frame of the video and identify where in the image a street sign is and identify what kind of sign it is. This section only needs to be run for training the YOLO model, can be skipped if you are using an existing .weights file.\n",
        "\n",
        "Data From: https://sid.erda.dk/public/archives/ff17dc924eba88d5d01a807357d6614c/published-archive.html"
      ],
      "metadata": {
        "id": "XbcPrjFV3biW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts the formatting of gt.txt to the proper format for training  the YOLOv3 model,\n",
        "# returns a list with [x_cen, y_cen, width, height] which are normalized to the dimensions of the input frame\n",
        "def yolo_coordinates(split_label):\n",
        "  width = int(split_label[3]) - int(split_label[1])\n",
        "  height = int(split_label[4]) - int(split_label[2])\n",
        "  x_cen = int(split_label[1]) + (width / 2)\n",
        "  y_cen = int(split_label[2]) + (height / 2)\n",
        "\n",
        "  # Normalize the pixel values\n",
        "  width = width / 1360\n",
        "  height = height / 800\n",
        "  x_cen = x_cen / 1360\n",
        "  y_cen = y_cen / 800\n",
        "\n",
        "  # Overwrite the values with new normalized ones in the YOLO order\n",
        "  split_label[1] = str(x_cen)\n",
        "  split_label[2] = str(y_cen)\n",
        "  split_label[3] = str(width)\n",
        "  split_label[4] = str(height)\n",
        "  return(split_label)"
      ],
      "metadata": {
        "id": "kvmNqtjpT37a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a dictionary where the key is the image name, and the values is a list\n",
        "# of the labels in a YOLO readable format\n",
        "sign_labels = \"gt.txt\"\n",
        "image_to_labels = {}\n",
        "\n",
        "f = open(sign_labels, \"r\")\n",
        "labels = f.readlines()\n",
        "for label in labels:\n",
        "  split_label = label.rstrip().split(\";\")\n",
        "  split_label = yolo_coordinates(split_label)\n",
        "  if split_label[0] in image_to_labels.keys():\n",
        "    image_to_labels[split_label[0]].append(split_label[5] + \" \" + split_label[1] + \" \" + split_label[2] + \" \" + split_label[3] + \" \" + split_label[4] + '\\n')\n",
        "  else:\n",
        "    image_to_labels[split_label[0]] = [(split_label[5] + \" \" + split_label[1] + \" \" + split_label[2] + \" \" + split_label[3] + \" \" + split_label[4] + '\\n')]\n",
        "f.close()"
      ],
      "metadata": {
        "id": "42Qg8D_e3oOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads through each image and creates a txt file of the same name where each\n",
        "# line is a YOLO readable label of an object in the image. Saves these txt\n",
        "# files to my google drive in the same directory as the images\n",
        "#\n",
        "# Only run once to put the txt files into google drive\n",
        "for image in image_to_labels.keys():\n",
        "  image_num = image[:-4]\n",
        "  path = '/content/gdrive/MyDrive/MLBoot/SignFinding/' + image_num + '.txt'\n",
        "  text = ''\n",
        "  for label in image_to_labels[image]:\n",
        "    text += label\n",
        "  with open(path, 'w') as f:\n",
        "    f.write(text)"
      ],
      "metadata": {
        "id": "RZktGQc5Xqxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creates a list of all images with label txt files and splits them into a train\n",
        "# and test set, with 80% of the images going to training\n",
        "data_sign = []\n",
        "for image_keys in image_to_labels.keys():\n",
        "  data_sign.append(image_keys)\n",
        "x_train, x_test = train_test_split(np.array(data_sign), test_size=0.2, shuffle=True)"
      ],
      "metadata": {
        "id": "EvLD2OnQesGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes each value and re-writes it as the absolute path to the file\n",
        "def split_to_txt(x):\n",
        "  text = ''\n",
        "  for val in x:\n",
        "    text += '/content/gdrive/MyDrive/MLBoot/SignFinding/' + (val + '\\n')\n",
        "  return text"
      ],
      "metadata": {
        "id": "urPT_OOigHMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the train and test sets and writes them to txt files where each line is the path to a file\n",
        "train_text = split_to_txt(x_train)\n",
        "test_text = split_to_txt(x_test)\n",
        "with open('/content/gdrive/MyDrive/MLBoot/train.txt', 'w') as f:\n",
        "  f.write(train_text)\n",
        "with open('/content/gdrive/MyDrive/MLBoot/test.txt', 'w') as f:\n",
        "  f.write(test_text)"
      ],
      "metadata": {
        "id": "u8FNKfY3gg7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check to ensure the T4 GPU is connected, will only run if connected to collabs T4 GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqMfmo00Txmh",
        "outputId": "91a1ac6a-d5ad-4ffb-a9de-e0bfbaab80de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Feb 15 02:17:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the darknet repository which will be used to train and assess the YOLO model\n",
        "!git clone https://github.com/AlexeyAB/darknet"
      ],
      "metadata": {
        "id": "gop2vWl_ikE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the current working directory to the darknet folder\n",
        "%cd darknet"
      ],
      "metadata": {
        "id": "AjhMhhucjPP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before constructing the darknet, edit the makefile to enable GPU, CUDNN, CUDNN_HALF, and OPENCV by changing parameters to 1 to improve performance."
      ],
      "metadata": {
        "id": "mcAL-xGDF0Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construct the darknet for training the model\n",
        "!make"
      ],
      "metadata": {
        "id": "94_k3UaujcPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and edit the yolo3 cfg file as specified in the ReadMe file in the darknet github repository:\n",
        "\n",
        "\n",
        "\n",
        "*   Comment out testing parameters and uncomment training ones\n",
        "*   Set max batches to (# of classes) * 2000, I have 43 classes\n",
        "*   Change steps to 80% and 90% of max batches\n",
        "*   Change the classes parameter to the number of classes at lines 610, 698, and 783\n",
        "*   Change filters to (# of classes + 5) * 3 at lines 603, 689, and 776\n",
        "\n",
        "\n",
        "Next you will need to construct a custon.names file and a detector.data file. Make sure that the formatting matches the ones I created. It is important to list the classes in the same order they are categorized within your file.\n",
        "\n"
      ],
      "metadata": {
        "id": "CJvr1zdFG995"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After downloading and editing the config file, retrieve initial pretrained weights for a YOLOv3 model\n",
        "!wget https://pjreddie.com/media/files/darknet53.conv.74"
      ],
      "metadata": {
        "id": "tNL_c4FvmO9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the YOLO model starting with the pretrained weights\n",
        "!./darknet detector train /content/gdrive/MyDrive/MLBoot/detector.data /content/gdrive/MyDrive/MLBoot/cfg/yolov3-custom.cfg darknet53.conv.74 -dont_show"
      ],
      "metadata": {
        "id": "koXvxMzLRVFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resume earlier training by using weights saved from initial training by changing the initial\n",
        "# weights from the darknet53.conv.74 to the saved .weights file from your initial training\n",
        "!./darknet detector train /content/gdrive/MyDrive/MLBoot/detector.data /content/gdrive/MyDrive/MLBoot/cfg/yolov3-custom.cfg /content/gdrive/MyDrive/MLBoot/Backup/yolov3-custom_last.weights -dont_show"
      ],
      "metadata": {
        "id": "Ma8OAv0-xblU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View average precision of the model on all of the classes\n",
        "!./darknet detector map /content/gdrive/MyDrive/MLBoot/detector.data /content/gdrive/MyDrive/MLBoot/cfg/yolov3-custom.cfg /content/gdrive/MyDrive/MLBoot/Backup/yolov3-custom_last.weights -dont_show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEwFw9Q3rkLb",
        "outputId": "76d93986-7517-4d51-cf49-83ffc151319c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CUDA-version: 12020 (12020), cuDNN: 8.9.6, CUDNN_HALF=1, GPU count: 1  \n",
            " CUDNN_HALF=1 \n",
            " OpenCV version: 4.5.4\n",
            " 0 : compute_capability = 750, cudnn_half = 1, GPU: Tesla T4 \n",
            "net.optimized_memory = 0 \n",
            "mini_batch = 1, batch = 16, time_steps = 1, train = 0 \n",
            "   layer   filters  size/strd(dil)      input                output\n",
            "   0 Create CUDA-stream - 0 \n",
            " Create cudnn-handle 0 \n",
            "conv     32       3 x 3/ 1    416 x 416 x   3 ->  416 x 416 x  32 0.299 BF\n",
            "   1 conv     64       3 x 3/ 2    416 x 416 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   2 conv     32       1 x 1/ 1    208 x 208 x  64 ->  208 x 208 x  32 0.177 BF\n",
            "   3 conv     64       3 x 3/ 1    208 x 208 x  32 ->  208 x 208 x  64 1.595 BF\n",
            "   4 Shortcut Layer: 1,  wt = 0, wn = 0, outputs: 208 x 208 x  64 0.003 BF\n",
            "   5 conv    128       3 x 3/ 2    208 x 208 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   6 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "   7 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "   8 Shortcut Layer: 5,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "   9 conv     64       1 x 1/ 1    104 x 104 x 128 ->  104 x 104 x  64 0.177 BF\n",
            "  10 conv    128       3 x 3/ 1    104 x 104 x  64 ->  104 x 104 x 128 1.595 BF\n",
            "  11 Shortcut Layer: 8,  wt = 0, wn = 0, outputs: 104 x 104 x 128 0.001 BF\n",
            "  12 conv    256       3 x 3/ 2    104 x 104 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  13 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  14 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  15 Shortcut Layer: 12,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  16 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  17 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  18 Shortcut Layer: 15,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  19 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  20 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  21 Shortcut Layer: 18,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  22 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  23 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  24 Shortcut Layer: 21,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  25 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  26 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  27 Shortcut Layer: 24,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  28 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  29 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  30 Shortcut Layer: 27,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  31 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  32 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  33 Shortcut Layer: 30,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  34 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            "  35 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            "  36 Shortcut Layer: 33,  wt = 0, wn = 0, outputs:  52 x  52 x 256 0.001 BF\n",
            "  37 conv    512       3 x 3/ 2     52 x  52 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  38 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  39 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  40 Shortcut Layer: 37,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  41 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  42 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  43 Shortcut Layer: 40,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  44 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  45 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  46 Shortcut Layer: 43,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  47 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  48 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  49 Shortcut Layer: 46,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  50 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  51 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  52 Shortcut Layer: 49,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  53 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  54 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  55 Shortcut Layer: 52,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  56 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  57 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  58 Shortcut Layer: 55,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  59 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  60 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  61 Shortcut Layer: 58,  wt = 0, wn = 0, outputs:  26 x  26 x 512 0.000 BF\n",
            "  62 conv   1024       3 x 3/ 2     26 x  26 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  63 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  64 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  65 Shortcut Layer: 62,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  66 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  67 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  68 Shortcut Layer: 65,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  69 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  70 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  71 Shortcut Layer: 68,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  72 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  73 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  74 Shortcut Layer: 71,  wt = 0, wn = 0, outputs:  13 x  13 x1024 0.000 BF\n",
            "  75 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  76 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  77 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  78 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  79 conv    512       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 512 0.177 BF\n",
            "  80 conv   1024       3 x 3/ 1     13 x  13 x 512 ->   13 x  13 x1024 1.595 BF\n",
            "  81 conv    144       1 x 1/ 1     13 x  13 x1024 ->   13 x  13 x 144 0.050 BF\n",
            "  82 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  83 route  79 \t\t                           ->   13 x  13 x 512 \n",
            "  84 conv    256       1 x 1/ 1     13 x  13 x 512 ->   13 x  13 x 256 0.044 BF\n",
            "  85 upsample                 2x    13 x  13 x 256 ->   26 x  26 x 256\n",
            "  86 route  85 61 \t                           ->   26 x  26 x 768 \n",
            "  87 conv    256       1 x 1/ 1     26 x  26 x 768 ->   26 x  26 x 256 0.266 BF\n",
            "  88 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  89 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  90 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  91 conv    256       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 256 0.177 BF\n",
            "  92 conv    512       3 x 3/ 1     26 x  26 x 256 ->   26 x  26 x 512 1.595 BF\n",
            "  93 conv    144       1 x 1/ 1     26 x  26 x 512 ->   26 x  26 x 144 0.100 BF\n",
            "  94 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "  95 route  91 \t\t                           ->   26 x  26 x 256 \n",
            "  96 conv    128       1 x 1/ 1     26 x  26 x 256 ->   26 x  26 x 128 0.044 BF\n",
            "  97 upsample                 2x    26 x  26 x 128 ->   52 x  52 x 128\n",
            "  98 route  97 36 \t                           ->   52 x  52 x 384 \n",
            "  99 conv    128       1 x 1/ 1     52 x  52 x 384 ->   52 x  52 x 128 0.266 BF\n",
            " 100 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 101 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 102 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 103 conv    128       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 128 0.177 BF\n",
            " 104 conv    256       3 x 3/ 1     52 x  52 x 128 ->   52 x  52 x 256 1.595 BF\n",
            " 105 conv    144       1 x 1/ 1     52 x  52 x 256 ->   52 x  52 x 144 0.199 BF\n",
            " 106 yolo\n",
            "[yolo] params: iou loss: mse (2), iou_norm: 0.75, obj_norm: 1.00, cls_norm: 1.00, delta_norm: 1.00, scale_x_y: 1.00\n",
            "Total BFLOPS 65.610 \n",
            "avg_outputs = 525081 \n",
            " Allocate additional workspace_size = 52.44 MB \n",
            "Loading weights from /content/gdrive/MyDrive/MLBoot/Backup/yolov3-custom_last.weights...\n",
            " seen 64, trained: 192 K-images (3 Kilo-batches_64) \n",
            "Done! Loaded 107 layers from weights-file \n",
            "\n",
            " calculation mAP (mean average precision)...\n",
            " Detection layer: 82 - type = 28 \n",
            " Detection layer: 94 - type = 28 \n",
            " Detection layer: 106 - type = 28 \n",
            "152\n",
            " detections_count = 1109, unique_truth_count = 250  \n",
            "class_id = 0, name = speed limit 20 (prohibitory), ap = 33.33%   \t (TP = 0, FP = 0) \n",
            "class_id = 1, name = speed limit 30 (prohibitory), ap = 63.57%   \t (TP = 11, FP = 6) \n",
            "class_id = 2, name = speed limit 50 (prohibitory), ap = 44.63%   \t (TP = 11, FP = 14) \n",
            "class_id = 3, name = speed limit 60 (prohibitory), ap = 43.33%   \t (TP = 5, FP = 9) \n",
            "class_id = 4, name = speed limit 70 (prohibitory), ap = 28.97%   \t (TP = 4, FP = 2) \n",
            "class_id = 5, name = speed limit 80 (prohibitory), ap = 70.26%   \t (TP = 12, FP = 15) \n",
            "class_id = 6, name = restriction ends 80 (other), ap = 75.00%   \t (TP = 2, FP = 1) \n",
            "class_id = 7, name = speed limit 100 (prohibitory), ap = 65.28%   \t (TP = 5, FP = 6) \n",
            "class_id = 8, name = speed limit 120 (prohibitory), ap = 48.62%   \t (TP = 4, FP = 2) \n",
            "class_id = 9, name = no overtaking (prohibitory), ap = 55.00%   \t (TP = 2, FP = 3) \n",
            "class_id = 10, name = no overtaking (trucks) (prohibitory), ap = 89.32%   \t (TP = 14, FP = 3) \n",
            "class_id = 11, name = priority at next intersection (danger), ap = 60.00%   \t (TP = 4, FP = 7) \n",
            "class_id = 12, name = priority road (other), ap = 85.40%   \t (TP = 16, FP = 4) \n",
            "class_id = 13, name = give way (other), ap = 97.06%   \t (TP = 14, FP = 3) \n",
            "class_id = 14, name = stop (other), ap = 81.82%   \t (TP = 9, FP = 0) \n",
            "class_id = 15, name = no traffic both ways (prohibitory), ap = 55.00%   \t (TP = 2, FP = 1) \n",
            "class_id = 16, name = no trucks (prohibitory), ap = 11.11%   \t (TP = 0, FP = 4) \n",
            "class_id = 17, name = no entry (other), ap = 100.00%   \t (TP = 2, FP = 0) \n",
            "class_id = 18, name = danger (danger), ap = 47.46%   \t (TP = 3, FP = 2) \n",
            "class_id = 19, name = bend left (danger), ap = 0.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 20, name = bend right (danger), ap = 6.67%   \t (TP = 0, FP = 0) \n",
            "class_id = 21, name = bend (danger), ap = 100.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 22, name = uneven road (danger), ap = 0.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 23, name = slippery road (danger), ap = 100.00%   \t (TP = 2, FP = 0) \n",
            "class_id = 24, name = road narrows (danger), ap = 0.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 25, name = construction (danger), ap = 21.30%   \t (TP = 2, FP = 6) \n",
            "class_id = 26, name = traffic signal (danger), ap = 0.00%   \t (TP = 0, FP = 1) \n",
            "class_id = 27, name = pedestrian crossing (danger), ap = 0.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 28, name = school crossing (danger), ap = 22.22%   \t (TP = 0, FP = 2) \n",
            "class_id = 29, name = cycles crossing (danger), ap = 20.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 30, name = snow (danger), ap = 0.00%   \t (TP = 0, FP = 2) \n",
            "class_id = 31, name = animals (danger), ap = 0.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 32, name = restriction ends (other), ap = 66.67%   \t (TP = 1, FP = 1) \n",
            "class_id = 33, name = go right (mandatory), ap = 50.00%   \t (TP = 1, FP = 1) \n",
            "class_id = 34, name = go left (mandatory), ap = 0.00%   \t (TP = 0, FP = 1) \n",
            "class_id = 35, name = go straight (mandatory), ap = 16.46%   \t (TP = 1, FP = 2) \n",
            "class_id = 36, name = go right or straight (mandatory), ap = 26.19%   \t (TP = 1, FP = 2) \n",
            "class_id = 37, name = go left or straight (mandatory), ap = 0.00%   \t (TP = 0, FP = 1) \n",
            "class_id = 38, name = keep right (mandatory), ap = 71.37%   \t (TP = 20, FP = 5) \n",
            "class_id = 39, name = keep left (mandatory), ap = 8.33%   \t (TP = 0, FP = 0) \n",
            "class_id = 40, name = roundabout (mandatory), ap = 100.00%   \t (TP = 1, FP = 0) \n",
            "class_id = 41, name = restriction ends (overtaking) (other), ap = 4.00%   \t (TP = 0, FP = 0) \n",
            "class_id = 42, name = restriction ends (overtaking (trucks)) (other), ap = 12.50%   \t (TP = 0, FP = 0) \n",
            "\n",
            " for conf_thresh = 0.25, precision = 0.58, recall = 0.60, F1-score = 0.59 \n",
            " for conf_thresh = 0.25, TP = 149, FP = 106, FN = 101, average IoU = 43.48 % \n",
            "\n",
            " IoU threshold = 50 %, used Area-Under-Curve for each unique Recall \n",
            " mean average precision (mAP@0.50) = 0.414156, or 41.42 % \n",
            "Total Detection Time: 6 Seconds\n",
            "\n",
            "Set -points flag:\n",
            " `-points 101` for MS COCO \n",
            " `-points 11` for PascalVOC 2007 (uncomment `difficult` in voc.data) \n",
            " `-points 0` (AUC) for ImageNet, PascalVOC 2010-2012, your custom dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My model was trained on 3000 iterations over four hours to achieve an average loss of 0.266"
      ],
      "metadata": {
        "id": "kOwBFdpyHXAJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the final video\n",
        "To create the final video, I need to extract the frames of the test video and transform them into forms that can be interpreted by my two models. First all frames are compiled into optical flows and sent to the speed model for a batch prediction. This output is then converted to a list, where the index of the list is correlated to the frame. Then, frames are individually fed into the YOLO model, where boxes are drawn around recognized signs and labeled. The YOLO model is only trained to recognize German street signs, so the footage takes place on German roads.\n",
        "\n",
        "\n",
        "Final Video is a Clip From: https://www.youtube.com/watch?v=sWiq7fPnRbE"
      ],
      "metadata": {
        "id": "-15y4jtR3KUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve trained speed model weights from earlier training and create\n",
        "# the speed model\n",
        "speed_model = tf.keras.models.load_model('/content/gdrive/MyDrive/SpeedModel/speed_model.keras')"
      ],
      "metadata": {
        "id": "SC94ApJuQ5IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The test video has overlay text at the bottom that is constantly updating,\n",
        "# this may interfere with the optical flow so grey boxes are placed over the\n",
        "# text. This method only applies to the specific video used for the final output.\n",
        "def cover_overlay(frame):\n",
        "  cv2.rectangle(frame, (25, 705), (315, 690), (100, 100, 100), cv2.FILLED)\n",
        "  cv2.rectangle(frame, (400, 705), (780, 690), (100, 100, 100), cv2.FILLED)"
      ],
      "metadata": {
        "id": "5U08HdAOSs2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes input file and converts its frames to optical flow. Optical flows are then\n",
        "# sent to the speed_model to get velocity predictions which are then exported.\n",
        "def get_final_video_labels(testing_data):\n",
        "  cap = cv2.VideoCapture(testing_data)\n",
        "  speed_model_input = []\n",
        "  while True:\n",
        "    ret, frame1 = cap.read() # Get the first frame using cv2\n",
        "    ret, frame2 = cap.read() # Get the second frame\n",
        "    if frame1 is None or frame2 is None: # Exit loop once out of frames\n",
        "      break\n",
        "    cover_overlay(frame1)\n",
        "    cover_overlay(frame2)\n",
        "    speed_model_input.append(frames_to_optical_flow(frame1, frame2))\n",
        "  speed_predictions = speed_model.predict(np.array(speed_model_input))\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "  return speed_predictions"
      ],
      "metadata": {
        "id": "Pw6wi7C80dIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts the speed predictions to a list of predicted speeds for each frame\n",
        "speed_p = get_final_video_labels(final_vid)\n",
        "frame_speeds = [elem[0] for elem in speed_p for i in range(2)]"
      ],
      "metadata": {
        "id": "DTOEHa3MAtpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rounds the output speed predictions to two decimal places and saves them to a new list\n",
        "speed_f = []\n",
        "for i in frame_speeds:\n",
        " speed_f.append(int(i * 100) / 100)"
      ],
      "metadata": {
        "id": "Xz7jmhhHY2qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters and values used for writing text onto the image frames\n",
        "vid_di = 416\n",
        "confThre = 0.5\n",
        "nmsThre = 0.3\n",
        "\n",
        "# List of all class names, index of list correlates to class ID\n",
        "classNames = ['speed limit 20', 'speed limit 30', 'speed limit 50',\n",
        "'speed limit 60', 'speed limit 70', 'speed limit 80',\n",
        "'restriction ends 80', 'speed limit 100', 'speed limit 120',\n",
        "'no overtaking', 'no overtaking (trucks)', 'priority at next intersection',\n",
        "'priority road', 'give way', 'stop', 'no traffic both ways',\n",
        "'no trucks', 'no entry', 'danger', 'bend left',\n",
        "'bend right', 'bend', 'uneven road', 'slippery road',\n",
        "'road narrows', 'construction', 'traffic signal', 'pedestrian crossing',\n",
        "'school crossing', 'cycles crossing', 'snow', 'animals',\n",
        "'restriction ends', 'go right', 'go left', 'go straight',\n",
        "'go right or straight', 'go left or straight', 'keep right',\n",
        "'keep left', 'roundabout', 'restriction ends (overtaking)',\n",
        "'restriction ends (overtaking trucks)']\n",
        "\n",
        "# Config and weights file from the YOLO training phase\n",
        "modelConfig = '/content/gdrive/MyDrive/MLBoot/cfg/yolov3-custom.cfg'\n",
        "modelWeights = '/content/gdrive/MyDrive/MLBoot/Backup/yolov3-custom_last.weights'"
      ],
      "metadata": {
        "id": "A9siOULZq7Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model using cv2 and the config and weight created earlier\n",
        "net = cv2.dnn.readNetFromDarknet(modelConfig, modelWeights)\n",
        "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
        "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
      ],
      "metadata": {
        "id": "Iq25jWH8s8lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Takes the output layer of the model and checks for highest class score\n",
        "# and if the confidence in that class exceeds the threshold converts the\n",
        "# dimensions of the box to cv2 usable format and puts the box and the label\n",
        "# on the frame\n",
        "def find_objects(outputs, frame):\n",
        "  height, width, channels = frame.shape\n",
        "  boxes = []\n",
        "  classes = []\n",
        "  confidence = []\n",
        "\n",
        "  # Iterate through output layers prediction scores for each class\n",
        "  for output in outputs:\n",
        "    for objs in outputs:\n",
        "      for obj in objs:\n",
        "        scores = obj[5:]\n",
        "        classIndex = np.argmax(scores)\n",
        "        class_conf = scores[classIndex]\n",
        "        if class_conf >= confThre:\n",
        "          box_w = int(obj[2] * width)\n",
        "          box_h = int(obj[3] * height)\n",
        "          box_x = int((obj[0] * width) - box_w/2)\n",
        "          box_y = int((obj[1] * height) - box_h/2)\n",
        "          boxes.append([box_x, box_y, box_w, box_h])\n",
        "          classes.append(classIndex)\n",
        "          confidence.append(float(class_conf))\n",
        "\n",
        "  # Index boxes and reduce to one box per sign recognition\n",
        "  indicies = cv2.dnn.NMSBoxes(boxes, confidence, confThre, nmsThre)\n",
        "\n",
        "  # Iterate through and draw boxes with labels and confidence levels\n",
        "  for i in indicies:\n",
        "    box = boxes[i]\n",
        "    x, y, w, h = box[0], box[1], box[2], box[3]\n",
        "    cv2.rectangle(frame, (x, y), (x + w, y + h), (36, 255, 12), 2)\n",
        "    cv2.putText(frame, f'{classNames[classes[i]].upper()} {int(confidence[i] * 100)}%', (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (36, 255, 12), 2)"
      ],
      "metadata": {
        "id": "XrzIc_fzv6AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializes video file for output\n",
        "result = cv2.VideoWriter('/content/MLBootfinalvid.avi',\n",
        "                         cv2.VideoWriter_fourcc(*'MJPG'),\n",
        "                         30, (1280, 720))"
      ],
      "metadata": {
        "id": "_09g9qUw_uR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reads the video frame by frame and runs object detection on each one\n",
        "# and adds boxes for detected objects. Prints the associated estimated velocity\n",
        "# on the frame as well and then writes it to the output file\n",
        "def object_detection(video_data, speed_labels):\n",
        "  frame_count = 0\n",
        "  cap = cv2.VideoCapture(video_data)\n",
        "  while True:\n",
        "    ret, frame = cap.read()\n",
        "    if frame is None or frame_count == len(speed_labels):\n",
        "      break\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1/255, (vid_di, vid_di), [0, 0, 0], swapRB=True, crop=False)\n",
        "    net.setInput(blob)\n",
        "    outputLayers = net.getUnconnectedOutLayersNames()\n",
        "    outputs = net.forward(outputLayers)\n",
        "    find_objects(outputs, frame)\n",
        "    cv2.rectangle(frame, (0, 0), (225, 80), (200, 200, 200), cv2.FILLED)\n",
        "    cv2.putText(frame, (str(speed_labels[frame_count]) + ' m/s'), (28, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "    result.write(frame)\n",
        "    frame_count += 1\n",
        "    if frame_count % 100 == 0:\n",
        "      print(frame_count)\n",
        "  cap.release()\n",
        "  result.release()\n",
        "  cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "HHa7hRrzuhyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This kernel runs the object detection model and draws the model output on\n",
        "# each frame, then the frame is written to a video file\n",
        "object_detection(final_vid, speed_f)"
      ],
      "metadata": {
        "id": "RRH6fECR3v_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frame Testing\n",
        "This section is used for testing how the models interact with input frames by outputting predictions and printing them onto a single frame."
      ],
      "metadata": {
        "id": "wF5wi89hTb39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final_input has 8140 frames\n",
        "test_frame = 8140\n",
        "testcap = cv2.VideoCapture('/content/final_input.mp4')\n",
        "total_frames = testcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "testcap.set(cv2.CAP_PROP_POS_FRAMES, test_frame)\n",
        "ret, testimg = testcap.read()\n",
        "blob = cv2.dnn.blobFromImage(testimg, 1/255, (vid_di, vid_di), [0, 0, 0], swapRB=True, crop=False)\n",
        "net.setInput(blob)\n",
        "outputLayers = net.getUnconnectedOutLayersNames()\n",
        "outputs = net.forward(outputLayers)\n",
        "find_objects(outputs, testimg)\n",
        "cv2.rectangle(testimg, (0, 0), (225, 80), (200, 200, 200), cv2.FILLED)\n",
        "cv2.putText(testimg, (str(test_frame) + ' m/s'), (28, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
        "cv2.rectangle(testimg, (25, 705), (315, 690), (100, 100, 100), cv2.FILLED)\n",
        "cv2.rectangle(testimg, (400, 705), (780, 690), (100, 100, 100), cv2.FILLED)\n",
        "testimg = cv2.cvtColor(testimg,cv2.COLOR_BGR2RGB)\n",
        "testcap.release()\n",
        "plt.imshow(testimg)"
      ],
      "metadata": {
        "id": "Zgg-j2Rouo6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Reflection\n",
        "The object detection model became surprisingly effective after giving it ample time to train. While it still missed some signs that were driven by, I believe that this recognition issue would be solved if the model was trained for more iterations. The speed detection model seemed to be a bit more sensitive to the quality of video used. Covering up the overlay text seemed to help but it still suffers from occasional spikes in inaccuracy. This is most likely due to the constant passing traffic in the video, making it difficult for the model to tell if the car is moving or if its surroundings are. It seems to be much more accurate during long drives down roads without incoming traffic. Overall, I am very happy with how effective these two models are.\n",
        "\n",
        "Video Output: https://youtu.be/fLxkel230tQ"
      ],
      "metadata": {
        "id": "g1sNL0H6U-EK"
      }
    }
  ]
}